import json
import os
import time

import requests
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv(os.path.join("..", ".env"))

# Definir las variables de entorno
auth_token = os.getenv("AUTH_TOKEN")


headers = {"Content-Type": "application/json", "Authorization": f"Bearer {auth_token}"}
url = "https://api.awanllm.com/v1/chat/completions"  # URL for the API

# Inicializar el cliente con la nueva interfaz
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


def preguntar_gpt(
    prompt,
    sys_prompt="Eres un asistente útil.",
    model="gpt-4o-mini",
    temperature=0.7,
    max_retries=3,
    timeout=60,
    rate_limit_delay=5,
):
    """
    Envía una solicitud a la API de OpenAI utilizando el modelo y parámetros indicados,
    utilizando la nueva interfaz de openai==1.0.0+.

    Args:
        sys_prompt (str): Mensaje de sistema para el modelo.
        prompt (str): Mensaje del usuario.
        model (str, optional): Modelo a utilizar. Defaults to "gpt-3.5-turbo".
        temperature (float, optional): Valor de temperatura para la generación. Defaults to 0.7.
        max_retries (int, optional): Número máximo de reintentos en caso de error. Defaults to 3.
        timeout (int, optional): Tiempo máximo de espera para la solicitud en segundos. Defaults to 60.
        rate_limit_delay (int, optional): Tiempo de espera al alcanzar el límite de solicitudes en segundos. Defaults to 5.

    Returns:
        str or None: Respuesta generada por el modelo o None si falla la solicitud.
    """
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": prompt},
                ],
                temperature=temperature,
                timeout=timeout,
            )
            # Acceder al contenido usando la nueva estructura del objeto
            return response.choices[0].message.content
        except Exception as e:
            # Puedes agregar lógica condicional según el mensaje de error si lo deseas.
            print(
                f"Intento {attempt + 1} fallido: {e}. Reintentando en {rate_limit_delay} segundos..."
            )
            time.sleep(rate_limit_delay)

    print("No se pudo obtener una respuesta tras varios intentos.")
    return None


def preguntar_llama(
    prompt,
    sys_prompt="Eres un asistente útil",
    max_retries=3,
    initial_delay=10,
    timeout=60,
    rate_limit_delay=5,
):
    """
    Sends a prompt to an AI model and retrieves the response.

    Args:
        sys_prompt (str): The system prompt to be sent to the AI model.
        prompt (str): The user prompt to be sent to the AI model.
        max_retries (int, optional): The maximum number of retries in case of errors. Defaults to 3.
        initial_delay (int, optional): The initial delay between retries in seconds. Defaults to 1.
        timeout (int, optional): The timeout for the HTTP request in seconds. Defaults to 60.
        rate_limit_delay (int, optional): The delay between retries in case of rate limiting errors. Defaults to 5.

    Returns:
        str: The response generated by the AI model.

    Raises:
        requests.exceptions.HTTPError: If an HTTP error occurs during the request.
        requests.exceptions.Timeout: If the request times out.
        Exception: If an unexpected error occurs.

    """
    payload = json.dumps(
        {
            "model": "Meta-Llama-3-8B-Instruct",
            "messages": [
                {"role": "system", "content": sys_prompt},
                {"role": "user", "content": prompt},
            ],
        }
    )

    for attempt in range(max_retries):
        try:
            response = requests.post(
                url, headers=headers, data=payload, timeout=timeout
            )
            response.raise_for_status()
            json_response = response.json()
            return json_response["choices"][0]["message"]["content"]
        except requests.exceptions.HTTPError as e:
            status_code = e.response.status_code
            if status_code == 429:
                print(
                    f"Intento {attempt + 1} fallido. Error: {e}. Demasiadas solicitudes."
                )
                delay = rate_limit_delay
            elif status_code == 502:
                print(f"Intento {attempt + 1} fallido. Error: {e}. Error de Gateway.")
                delay = initial_delay
            else:
                print(f"Intento {attempt + 1} fallido. Error: {e}")
                break  # Salir del bucle si hay un error HTTP que no sea 429 o 502

            print(f"Reintentando en {delay} segundos...")
            time.sleep(delay)
        except requests.exceptions.Timeout as e:
            print(f"Intento {attempt + 1} fallido por tiempo de espera excedido: {e}")
            if attempt < max_retries - 1:
                print("Reintentando...")
            time.sleep(initial_delay * (2**attempt))
        except Exception as e:
            print(f"Error no esperado: {e}")
            if attempt == max_retries - 1:
                return None  # Retornar None solo después de todos los intentos

    print("No se pudo completar la solicitud después de varios intentos.")
    return None
