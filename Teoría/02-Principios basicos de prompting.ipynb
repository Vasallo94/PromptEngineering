{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Principios básicos del prompting\n",
        "\n",
        "1. Los prompts deben ser sencillos y claros. No es necesario escribir un párrafo entero para obtener una respuesta.\n",
        "2. Dale al modelo tiempo para \"pensar\" en la respuesta. No le des un prompt y esperes una respuesta inmediata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "import sys\n",
        "sys.path.append('../utils/')\n",
        "from funciones import preguntar_llama, preguntar_gpt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tácticas\n",
        "\n",
        "1. Utiliza delimitadores para indicar cómo distinguir partes del input.\n",
        "    Delimitadores pueden ser ```, \"\"\", < >, `<tag> </tag>`, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Respuesta GPT:** A menor temperatura en un LLM, los resultados son más deterministas y certeros, mientras que incrementarla permite mayor variabilidad y creatividad, siendo recomendable usar temperaturas bajas para tareas objetivas y altas para actividades creativas, aunque esto último puede llevar a alucinaciones."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Respuesta de LLAMA:** Aumentar la temperatura en un Modelo de Lenguaje de Gran Escala (LLM) introduce variabilidad en las respuestas y promueve resultados más creativos."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Definimos el texto\n",
        "text = \"\"\"En esencia, a menor temperatura en un Modelo de Lenguaje de Gran Escala (LLM), los resultados tienden a ser más deterministas. Esto implica que el modelo optará con más frecuencia por el siguiente token de mayor probabilidad. Incrementar la temperatura introduce mayor variabilidad en las respuestas, lo que promueve resultados más variados o creativos. Básicamente, al elevar la temperatura, se incrementa la posibilidad de elegir diferentes tokens como opciones. Desde un punto de vista práctico, en tareas como preguntas y respuestas basadas en información objetiva, se sugiere utilizar un valor de temperatura reducido para lograr respuestas más certeras y breves. Sin embargo, en la creación de poesía o en otras actividades creativas, puede resultar ventajoso aumentar el valor de la temperatura. El aumento en la temperatura puede generar alucinaciones.\"\"\"\n",
        "\n",
        "# Definimos el prompt\n",
        "prompt = f\"Resume el texto delimitado por las triples comillas en una sola oración: {text}\"\n",
        "\n",
        "# Obtenemos la respuesta\n",
        "response = preguntar_gpt(prompt)\n",
        "display(Markdown(f\"**Respuesta GPT:** {response}\"))\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "display(Markdown(f\"**Respuesta de LLAMA:** {preguntar_llama(prompt=prompt)}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. JSON, HTML, XML, YAML, etc. son formatos que el modelo puede entender y que pueden ayudar a estructurar el input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Respuesta GPT:** Aquí tienes una lista de tres libros inventados en formato JSON:\n",
            "\n",
            "```json\n",
            "[\n",
            "    {\n",
            "        \"book_id\": 1,\n",
            "        \"título\": \"El susurro del viento\",\n",
            "        \"autor\": \"María López\",\n",
            "        \"género\": \"Fantasía\"\n",
            "    },\n",
            "    {\n",
            "        \"book_id\": 2,\n",
            "        \"título\": \"Caminos de acero\",\n",
            "        \"autor\": \"José Martínez\",\n",
            "        \"género\": \"Ciencia Ficción\"\n",
            "    },\n",
            "    {\n",
            "        \"book_id\": 3,\n",
            "        \"título\": \"Sombras en la oscuridad\",\n",
            "        \"autor\": \"Ana García\",\n",
            "        \"género\": \"Thriller\"\n",
            "    }\n",
            "]\n",
            "```\n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "**Respuesta de LLAMA:** {\"book_id\":1,\"título\":\"La Ciudad Encubierta\",\"autor\":\"Jorge Silva\",\"género\":\"Novela Policial\"} \n",
            "\n",
            "{\"book_id\":2,\"título\":\"El Llamado de los Elementos\",\"autor\":\"Ana Gómez\",\"género\":\"Fantasía y Aventuras\"} \n",
            "\n",
            "{\"book_id\":3,\"título\":\"El Club de las Almas Perdidas\",\"autor\":\"Alberto Martínez\",\"género\":\"Terror y Misterio\"}\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Genera una lista de tres títulos de libros inventados junto con sus autores y géneros. Proporciónelos en formato JSON con las siguientes claves: book_id, título, autor, género\"\"\"\n",
        "response = preguntar_gpt(prompt)\n",
        "\n",
        "print(f\"**Respuesta GPT:** {response}\")\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "print(f\"**Respuesta de LLAMA:** {preguntar_llama(prompt=prompt)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Pídele al modelo que verifique si se están cumpliendo ciertas condiciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Respuesta GPT:** No se proporcionaron pasos."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Respuesta de LLAMA:** Paso 1 - Asegúrate de que la máquina esté encendida y lista para usar.\n",
              "\n",
              "Paso 2 - Muele los granos de café según el grosor deseado y colócalos en el portafiltro.\n",
              "\n",
              "Paso 3 - Compacta el café con el tampador para garantizar una extracción uniforme.\n",
              "\n",
              "Paso 4 - Ajusta la cantidad de café según tu preferencia y coloca el portafiltro en la máquina.\n",
              "\n",
              "Paso 5 - Activa la máquina y espera a que el café se extraiga lentamente, disfrutando del aroma que llena el aire."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "text_1 = \"\"\"Preparar una exquisita taza de café en una máquina espresso es un proceso sencillo. En primer lugar, asegúrate de que la máquina esté encendida y lista para usar. Luego, muele los granos de café según el grosor deseado y colócalos en el portafiltro. Compacta el café con el tampador para garantizar una extracción uniforme. Ajusta la cantidad de café según tu preferencia y coloca el portafiltro en la máquina. Activa la máquina y espera a que el café se extraiga lentamente, disfrutando del aroma que llena el aire. ¡Y voilà! Disfruta de tu aromático y delicioso café espresso.\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Vas a recibir un texto delimitado por triples comillas.\n",
        "Si contiene una secuencia de instrucciones, reescribe esas instrucciones en el siguiente formato:\n",
        "\n",
        "Paso 1 - ...\\n\n",
        "Paso 2 - ...\\n\n",
        "…\\n\n",
        "Paso N - ...\\n\n",
        "\n",
        "Si el texto no contiene una secuencia de instrucciones, entonces simplemente escribe \\\"No se proporcionaron pasos.\\\"\n",
        "\n",
        "Texto:\n",
        "```{text_1}```\n",
        "\"\"\"\n",
        "\n",
        "response = preguntar_gpt(prompt)\n",
        "display(Markdown(f\"**Respuesta GPT:** {response}\"))\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "display(Markdown(f\"**Respuesta de LLAMA:** {preguntar_llama(prompt=prompt)}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Few-shot prompting: Utiliza ejemplos para que el modelo entienda mejor lo que quieres."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Respuesta GPT:** No se proporcionaron pasos."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Respuesta de LLAMA:** No se proporcionaron pasos. El texto no contiene una secuencia de instrucciones, sino más bien información sobre comunicación humana."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "text_1 = \"\"\"Los humanos son seres sociales por naturaleza y, por lo tanto, la comunicación es una parte esencial de la vida cotidiana. La comunicación efectiva puede ser verbal o no verbal, y ambas formas son igualmente importantes. Algunos estudios sugieren que el 93% de la comunicación es no verbal, lo que significa que el lenguaje corporal, las expresiones faciales y otros gestos juegan un papel crucial en la forma en que nos comunicamos. Por otro lado, la comunicación verbal también es fundamental, ya que nos permite expresar nuestros pensamientos, sentimientos y emociones de manera clara y concisa.\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Vas a recibir un texto delimitado por triples comillas.\n",
        "Si contiene una secuencia de instrucciones, reescribe esas instrucciones en el siguiente formato:\n",
        "\n",
        "Paso 1 - ...\\n\n",
        "Paso 2 - ...\\n\n",
        "…\\n\n",
        "Paso N - ...\\n\n",
        "\n",
        "Si el texto no contiene una secuencia de instrucciones, entonces simplemente escribe \\\"No se proporcionaron pasos.\\\"\n",
        "\n",
        "Texto:\n",
        "```{text_1}```\n",
        "\"\"\"\n",
        "\n",
        "response = preguntar_gpt(prompt)\n",
        "display(Markdown(f\"**Respuesta GPT:** {response}\"))\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "display(Markdown(f\"**Respuesta de LLAMA:** {preguntar_llama(prompt=prompt)}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Respuesta GPT:** No se proporcionaron pasos."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Respuesta de LLAMA:** No se proporcionaron pasos. El texto parece ser una descripción general sobre la importancia de la comunicación en la vida cotidiana, más que una secuencia de instrucciones para realizar algo específico."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "text_1 = \"\"\"Los humanos son seres sociales por naturaleza y, por lo tanto, la comunicación es una parte esencial de la vida cotidiana. La comunicación efectiva puede ser verbal o no verbal, y ambas formas son igualmente importantes. Algunos estudios sugieren que el 93% de la comunicación es no verbal, lo que significa que el lenguaje corporal, las expresiones faciales y otros gestos juegan un papel crucial en la forma en que nos comunicamos. Por otro lado, la comunicación verbal también es fundamental, ya que nos permite expresar nuestros pensamientos, sentimientos y emociones de manera clara y concisa.\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Vas a recibir un texto delimitado por triples comillas.\n",
        "Si contiene una secuencia de instrucciones, reescribe esas instrucciones en el siguiente formato:\n",
        "\n",
        "Paso 1 - ...\\n\n",
        "Paso 2 - ...\\n\n",
        "…\\n\n",
        "Paso N - ...\\n\n",
        "\n",
        "Si el texto no contiene una secuencia de instrucciones, entonces simplemente escribe \\\"No se proporcionaron pasos.\\\"\n",
        "\n",
        "Texto:\n",
        "```{text_1}```\n",
        "\"\"\"\n",
        "\n",
        "response = preguntar_gpt(prompt)\n",
        "display(Markdown(f\"**Respuesta GPT:** {response}\"))\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "display(Markdown(f\"**Respuesta de LLAMA:** {preguntar_llama(prompt=prompt)}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Respuesta GPT:** <abuelo>: La semilla que se entierra en la tierra oscura encuentra la luz del sol a través de la adversidad; el bambú que se dobla ante el viento fuerte no se quiebra, sino que se fortalece; el fénix renace de las cenizas, recordándonos que cada final es solo un nuevo comienzo."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Respuesta de LLAMA:** <abuelo>: Ah, joven mío, te hablaré de las raíces que alimentan las montañas. El arquitecto del templo más majestuoso esculpió el pilar con la base más débil; sin embargo, en su perseverancia y paciencia, logró elevar hacia las alturas una estructura donde cada piedra cuenta, un reflejo de lo que el mundo exterior no siempre comprende. La resiliencia también encuentra sus raíces en pequeños comienzos; es ahí donde el valor se define, no por la fuerza o el tamaño del final logrado, sino por la valentía mostrada ante los obstáculos y el trabajo diario en perseguir un sueño."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "Tu tarea es responder de manera consistente.\n",
        "\n",
        "<hijo>: Enséñame sobre la paciencia.\n",
        "\n",
        "<abuelo>: El río que talla el valle más profundo fluye desde una modesta fuente; la sinfonía más grandiosa se origina a partir de una sola nota; el tapiz más intrincado comienza con un hilo solitario.\n",
        "\n",
        "<hijo>: Enséñame sobre la resiliencia.\n",
        "\"\"\"\n",
        "response = preguntar_gpt(prompt)\n",
        "display(Markdown(f\"**Respuesta GPT:** {response}\"))\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "display(Markdown(f\"**Respuesta de LLAMA:** {preguntar_llama(prompt=prompt)}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Principio 2: Darle tiempo al modelo para \"pensar\" en la respuesta.\n",
        "\n",
        "Táctica: Especifica los pasos requeridos para completar la tarea."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Respuesta GPT para el prompt 1:** 1 - Los hermanos Mario y Sara se aventuraron en un bosque en busca de un tesoro perdido, pero se encontraron con trampas que los pusieron en peligro, aunque lograron salir y continuaron su búsqueda con renovado entusiasmo.\n",
            "\n",
            "2 - Les frères Mario et Sara se sont aventurés dans une forêt à la recherche d'un trésor perdu, mais ils ont été confrontés à des pièges qui les ont mis en danger, bien qu'ils aient réussi à s'en sortir et ont poursuivi leur quête avec un nouvel enthousiasme.\n",
            "\n",
            "3 - Mario, Sara\n",
            "\n",
            "4 - \n",
            "```json\n",
            "{\n",
            "  \"french_summary\": \"Les frères Mario et Sara se sont aventurés dans une forêt à la recherche d'un trésor perdu, mais ils ont été confrontés à des pièges qui les ont mis en danger, bien qu'ils aient réussi à s'en sortir et ont poursuivi leur quête avec un nouvel enthousiasme.\",\n",
            "  \"num_names\": 2\n",
            "}\n",
            "```\n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "**Respuesta de LLAMA para el prompt 1:** **Resumen del texto en 1 frase:**\n",
            "\n",
            "Los hermanos Mario y Sara se aventuraban en busca del tesoro perdido en un bosque tranquilo pero sus planes fueron truncados al caer en trampas, lo que les enseñó una nueva perspectiva sobre la aventura.\n",
            "\n",
            "**Traducción al francés:**\n",
            "Les frères Mario et Sara s'aventuraient en quête du trésor perdu dans un bois tranquille mais leurs plans furent interrompus tomber dans des pièges, ce qui leur enseigna une nouvelle perspective sur l'aventure.\n",
            "\n",
            "\n",
            "**Lista de nombres en francés:**\n",
            "\n",
            "- Mario\n",
            "\n",
            "\n",
            "**Objeto JSON:**\n",
            "{\n",
            "\"french_summary\": \"Les frères Mario et Sara s'aventuraient en quête du trésor perdu dans un bois tranquille mais leurs plans furent interrompus tomber dans des pièges, ce qui leur enseigna une nouvelle perspective sur l'aventure\",\n",
            "\"num_names\": 1\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"\n",
        "En un tranquilo bosque, los hermanos Mario y Sara se aventuraron en busca de un antiguo tesoro perdido. Mientras caminaban entre los árboles, riendo y compartiendo historias, un ruido extraño los detuvo de repente. MArio, intrigado, se acercó para investigar y accidentalmente activó una trampa oculta. Una red se desplegó atrapándolo, y Sara, al intentar liberarlo, cayó en un foso cercano.\n",
        "\n",
        "Afortunadamente, ninguno de los dos resultó gravemente herido. Juntos, encontraron la manera de salir de sus respectivas trampas y regresaron a casa con algunas raspaduras y moretones. Sin embargo, en lugar de desanimarse, se abrazaron con alivio y se rieron de su mala suerte.\n",
        "\n",
        "Decidieron que la próxima vez serían más cuidadosos, pero su determinación por descubrir el tesoro perdido no disminuyó. Con una nueva perspectiva sobre la aventura, continuaron explorando el bosque con entusiasmo renovado, emocionados por lo que el destino les depararía.\n",
        "\"\"\"\n",
        "\n",
        "prompt_1 = f\"\"\"\n",
        "Realiza las siguientes acciones: \\n\n",
        "1 - Resumir el siguiente texto delimitado por triple comillas con 1 frase.\\n\n",
        "2 - Traduzca el resumen al francés.\\n\n",
        "3 - Listar cada nombre en el resumen en francés.\\n\n",
        "4 - Generar un objeto json que contenga lo siguiente\\n\n",
        "claves: french_summary, num_names.\\n\n",
        "\n",
        "Separa tus respuestas con saltos de línea.\\n\n",
        "\n",
        "Texto:\\n\n",
        "```{text}```\\n\n",
        "\"\"\"\n",
        "response = preguntar_gpt(prompt_1)\n",
        "print(f\"**Respuesta GPT para el prompt 1:** {response}\")\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "print(f\"**Respuesta de LLAMA para el prompt 1:** {preguntar_llama(prompt=prompt_1)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pide el resultado en un formato específico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Respuesta GPT para el prompt 2:** Resumen: Los hermanos Mario y Sara, en busca de un tesoro perdido, activan accidentalmente trampas en el bosque, pero logran salir y deciden seguir explorando con entusiasmo.\n",
            "\n",
            "Traducción: Les frères Mario et Sara, à la recherche d'un trésor perdu, activent accidentellement des pièges dans la forêt, mais parviennent à s'en sortir et décident de continuer à explorer avec enthousiasme.\n",
            "\n",
            "Nombres: [\"Mario\", \"Sara\"]\n",
            "\n",
            "JSON de salida: {\n",
            "  \"french_summary\": \"Les frères Mario et Sara, à la recherche d'un trésor perdu, activent accidentellement des pièges dans la forêt, mais parviennent à s'en sortir et décident de continuer à explorer avec enthousiasme.\",\n",
            "  \"num_names\": 2\n",
            "}\n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "**Respuesta de LLAMA para el prompt 2:** Texto: <texto a resumir>\n",
            "Resumen: Los hermanos Mario y Sara se aventuraban en busca del tesoro perdido, pero sus intentos fueron detenidos por trampas ocultas, resultando con raspaduras y moretones, pero no desanimados.\n",
            "Traducción: Les frères Mario et Sara s'avançaient à la recherche du trésor perdu, mais leurs tentatives furent empêchées par des pièges cachés, se produisant avec des éraflures et des ecchymoses, mais pas découragés.\n",
            "Nombres: [Mario, Sara]\n",
            "JSON de salida: \n",
            "{\n",
            "  \"french_summary\": \"Les frères Mario et Sara s'avançaient à la recherche du trésor perdu, mais leurs tentatives furent empêchées par des pièges cachés, se produisant avec des éraflures et des ecchymoses, mais pas découragés.\",\n",
            "  \"num_names\": 2\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "prompt_2 = f\"\"\"\n",
        "tu tarea consiste en realizar las siguientes acciones: \\n\n",
        "1 - Resumir el siguiente texto delimitado por <> con 1 frase.\\n\n",
        "2 - Traduzca el resumen al francés.\\n\n",
        "3 - Listar cada nombre en el resumen en francés.\\n\n",
        "4 - Dar salida a un objeto json que contenga las siguientes claves: french_summary, num_names.\\n\n",
        "\n",
        "Utilice el siguiente formato:\\n\n",
        "Texto: <texto a resumir>\\n\n",
        "Resumen: <resumen>\\n\n",
        "Traducción: <traducción del resumen>\\n\n",
        "Nombres: <lista de nombres en resumen>\\n\n",
        "JSON de salida: <json con resumen y número_nombres>\\n\n",
        "\n",
        "Texto: <{text}>\\n\n",
        "\"\"\"\n",
        "response = preguntar_gpt(prompt_2)\n",
        "print(f\"**Respuesta GPT para el prompt 2:** {response}\")\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "print(f\"**Respuesta de LLAMA para el prompt 2:** {preguntar_llama(prompt=prompt_2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Táctica 2: Pedir al modelo que elabore su propia solución antes de llegar a una conclusión precipitada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Limitaciones del modelo: Alucinaciones\n",
        "\n",
        "Apple es una compañía de tecnología real pero iCan no es un producto real de Apple. Sin embargo, el modelo puede alucinar que iCan es un producto real de Apple y devolvernos una respuesta que no es cierta.\n",
        "\n",
        "Los modelos cada vez responden mejor evitando las alucinaciones, esta pregunta el año pasado hubiera sido un ejemplo perfecto de alucinación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Respuesta GPT:** Hasta la fecha de mi último conocimiento en octubre de 2023, no tengo información sobre un dispositivo llamado \"iCan\" de Apple. Es posible que se trate de un producto nuevo o de un rumor que surgió después de esa fecha. Apple ha estado innovando continuamente en la categoría de dispositivos portátiles, incluyendo mejoras en el Apple Watch, AirPods y otros dispositivos.\n",
              "\n",
              "Si \"iCan\" es un nuevo dispositivo, sería útil revisar las fuentes oficiales de Apple o noticias recientes para obtener información actualizada sobre sus características, funciones y aplicaciones. Si tienes más detalles o contexto sobre el iCan, estaré encantado de ayudarte con la información que tengo."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Respuesta de LLAMA:** Lo siento, pero no tengo información actualizada sobre un dispositivo llamado \"iCan\" lanzado por Apple. Sin embargo, puedo hacerte un resumen de los dispositivos y tecnologías más recientes desarrolladas por la compañía.\n",
              "\n",
              "Apple ha lanzado varios dispositivos innovadores en los últimos años. Algunos ejemplos incluyen:\n",
              "\n",
              "1. **iPhone 12 Series (2020)**: Fue uno de los lanzamientos importantes del año 2020, caracterizados por su pantalla OLED más amplia y resistente, además de mejoras significativas en la cámara.\n",
              "2. **Apple Watch Series 7 (2021)**: Esta edición mejoró la pantalla y el sistema operativo del reloj inteligente, sumando una experiencia más completa para los usuarios.\n",
              "3. **iPad Air (4ª Generación) (2020)**: Esta actualización ofreció procesamiento más rápido, mejores cámaras y un designo elegante.\n",
              "4. **AirPods Pro (2019)**: Son auriculares inalámbricos con cancelación activa de ruido (ANC), brindando una experiencia sonora más impecable.\n",
              "\n",
              "No obstante, si te refieres a la última innovación significativa en dispositivos portátiles de Apple que yo sepa, podría ser el lanzamiento de modelos mejorados de iPad Air o iPad Pro con tecnología M1 y posteriormente los iPads Pro con M2. Estos incluyen una mayor velocidad y eficiencia gracias a la procesadora M-Series desarrollada por Apple.\n",
              "\n",
              "Para tener acceso a información más actualizada sobre cualquier dispositivo nuevo, te recomendaría visitar el sitio oficial de Apple o sus canales sociales donde se comunican noticias importantes y lanzamientos en vivo.\n",
              "\n",
              "¿Es algo específico lo que buscas saber sobre iCan? Tal vez pueda proporcionarte más contexto si supieras qué \"iCan\" podría significar."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "Háblame sobre el iCan, la última innovación en tecnología de dispositivos portátiles de Apple.\n",
        "\"\"\"\n",
        "response = preguntar_gpt(prompt)\n",
        "display(Markdown(f\"**Respuesta GPT:** {response}\"))\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "display(Markdown(f\"**Respuesta de LLAMA:** {preguntar_llama(prompt=prompt)}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Respuesta GPT:** 2332 multiplicado por 9450 es igual a 22,029,600."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Respuesta de LLAMA:** Para multiplicar 2332 por 9450, te ayudaré paso a paso:\n",
              "\n",
              "1. Multiplicar los números enteros (sin comas):\n",
              "   2332 x 9450 = ?\n",
              "\n",
              "2. Para calcular esto, puedo utilizar una calculadora mental o escribirlo en un papel para que no me equivoque.\n",
              " \n",
              "En este caso, te diré lo que haría:\n",
              "Primero multiplicaría la parte entera de ambos números: 2332 por 9000.\n",
              "\n",
              "Eso sería:\n",
              "23320000 \n",
              "\n",
              "Luego restaría los factores de las decenas:\n",
              "- Las decenas del primer número son 30, \n",
              "pero como el segundo es un múltiplo de 100 (que es igual a 10^2) tenemos que multiplicarlos: \n",
              "30 x 40 = 1200\n",
              "Ahora resta eso:\n",
              "\n",
              "23320000 - 1200\n",
              "\n",
              "Restando esto:\n",
              "\n",
              "23317980"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Solución real para la multiplicación: 2203740\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "Multiplica 2332 por 9450.\n",
        "\"\"\"\n",
        "# Los modelos cada vez son más inteligentes, pero aún no son capaces de hacer cálculos matemáticos complejos.\n",
        "\n",
        "response = preguntar_gpt(prompt)\n",
        "display(Markdown(f\"**Respuesta GPT:** {response}\"))\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "display(Markdown(f\"**Respuesta de LLAMA:** {preguntar_llama(prompt=prompt)}\"))\n",
        "print(f\"Solución real para la multiplicación: {2332*945}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tu turno de experimentar!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
