{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Principios básicos del prompting\n",
        "\n",
        "1. Los prompts deben ser sencillos y claros. No es necesario escribir un párrafo entero para obtener una respuesta.\n",
        "2. Dale al modelo tiempo para \"pensar\" en la respuesta. No le des un prompt y esperes una respuesta inmediata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'openai'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      4\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../utils/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunciones\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preguntar_llama, preguntar_gpt\n",
            "File \u001b[0;32m~/Clases_Pontia/PromptEngineering/Teoría/../utils/funciones.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m      9\u001b[0m load_dotenv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.env\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Definir las variables de entorno\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openai'"
          ]
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "import sys\n",
        "sys.path.append('../utils/')\n",
        "from funciones import preguntar_llama, preguntar_gpt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tácticas\n",
        "\n",
        "1. Utiliza delimitadores para indicar cómo distinguir partes del input.\n",
        "    Delimitadores pueden ser ```, \"\"\", < >, `<tag> </tag>`, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definimos el texto\n",
        "text = \"\"\"En esencia, a menor temperatura en un Modelo de Lenguaje de Gran Escala (LLM), los resultados tienden a ser más deterministas. Esto implica que el modelo optará con más frecuencia por el siguiente token de mayor probabilidad. Incrementar la temperatura introduce mayor variabilidad en las respuestas, lo que promueve resultados más variados o creativos. Básicamente, al elevar la temperatura, se incrementa la posibilidad de elegir diferentes tokens como opciones. Desde un punto de vista práctico, en tareas como preguntas y respuestas basadas en información objetiva, se sugiere utilizar un valor de temperatura reducido para lograr respuestas más certeras y breves. Sin embargo, en la creación de poesía o en otras actividades creativas, puede resultar ventajoso aumentar el valor de la temperatura. El aumento en la temperatura puede generar alucinaciones.\"\"\"\n",
        "\n",
        "# Definimos el prompt\n",
        "prompt = f\"Resume el texto delimitado por las triples comillas en una sola oración: {text}\"\n",
        "\n",
        "# Obtenemos la respuesta\n",
        "response = preguntar_gpt(prompt)\n",
        "display(Markdown(f\"**Respuesta GPT:** {response}\"))\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "display(Markdown(f\"**Respuesta de LLAMA:** {preguntar_llama('Eres un asistente de IA cuyo trabajo es responder a lo que te está pidiendo el usuario de la forma más fiel al prompt', prompt=prompt)}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. JSON, HTML, XML, YAML, etc. son formatos que el modelo puede entender y que pueden ayudar a estructurar el input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"\"\"Genera una lista de tres títulos de libros inventados junto con sus autores y géneros. Proporciónelos en formato JSON con las siguientes claves: book_id, título, autor, género\"\"\"\n",
        "response = preguntar_gpt(prompt)\n",
        "\n",
        "print(f\"**Respuesta GPT:** {response}\")\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "print(f\"**Respuesta de LLAMA:** {preguntar_llama('Eres un asistente de IA cuyo trabajo es responder a lo que te está pidiendo el usuario de la forma más fiel al prompt', prompt=prompt)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Pídele al modelo que verifique si se están cumpliendo ciertas condiciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_1 = \"\"\"Preparar una exquisita taza de café en una máquina espresso es un proceso sencillo. En primer lugar, asegúrate de que la máquina esté encendida y lista para usar. Luego, muele los granos de café según el grosor deseado y colócalos en el portafiltro. Compacta el café con el tampador para garantizar una extracción uniforme. Ajusta la cantidad de café según tu preferencia y coloca el portafiltro en la máquina. Activa la máquina y espera a que el café se extraiga lentamente, disfrutando del aroma que llena el aire. ¡Y voilà! Disfruta de tu aromático y delicioso café espresso.\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Vas a recibir un texto delimitado por triples comillas.\n",
        "Si contiene una secuencia de instrucciones, reescribe esas instrucciones en el siguiente formato:\n",
        "\n",
        "Paso 1 - ...\\n\n",
        "Paso 2 - ...\\n\n",
        "…\\n\n",
        "Paso N - ...\\n\n",
        "\n",
        "Si el texto no contiene una secuencia de instrucciones, entonces simplemente escribe \\\"No se proporcionaron pasos.\\\"\n",
        "\n",
        "Texto:\n",
        "```{text_1}```\n",
        "\"\"\"\n",
        "\n",
        "response = preguntar_gpt(prompt)\n",
        "display(Markdown(f\"**Respuesta GPT:** {response}\"))\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "display(Markdown(f\"**Respuesta de LLAMA:** {preguntar_llama('Eres un asistente de IA cuyo trabajo es responder a lo que te está pidiendo el usuario de la forma más fiel al prompt', prompt=prompt)}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Few-shot prompting: Utiliza ejemplos para que el modelo entienda mejor lo que quieres."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_1 = \"\"\"Los humanos son seres sociales por naturaleza y, por lo tanto, la comunicación es una parte esencial de la vida cotidiana. La comunicación efectiva puede ser verbal o no verbal, y ambas formas son igualmente importantes. Algunos estudios sugieren que el 93% de la comunicación es no verbal, lo que significa que el lenguaje corporal, las expresiones faciales y otros gestos juegan un papel crucial en la forma en que nos comunicamos. Por otro lado, la comunicación verbal también es fundamental, ya que nos permite expresar nuestros pensamientos, sentimientos y emociones de manera clara y concisa.\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Vas a recibir un texto delimitado por triples comillas.\n",
        "Si contiene una secuencia de instrucciones, reescribe esas instrucciones en el siguiente formato:\n",
        "\n",
        "Paso 1 - ...\\n\n",
        "Paso 2 - ...\\n\n",
        "…\\n\n",
        "Paso N - ...\\n\n",
        "\n",
        "Si el texto no contiene una secuencia de instrucciones, entonces simplemente escribe \\\"No se proporcionaron pasos.\\\"\n",
        "\n",
        "Texto:\n",
        "```{text_1}```\n",
        "\"\"\"\n",
        "\n",
        "response = preguntar_gpt(prompt)\n",
        "display(Markdown(f\"**Respuesta GPT:** {response}\"))\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "display(Markdown(f\"**Respuesta de LLAMA:** {preguntar_llama('Eres un asistente de IA cuyo trabajo es responder a lo que te está pidiendo el usuario de la forma más fiel al prompt', prompt=prompt)}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_1 = \"\"\"Los humanos son seres sociales por naturaleza y, por lo tanto, la comunicación es una parte esencial de la vida cotidiana. La comunicación efectiva puede ser verbal o no verbal, y ambas formas son igualmente importantes. Algunos estudios sugieren que el 93% de la comunicación es no verbal, lo que significa que el lenguaje corporal, las expresiones faciales y otros gestos juegan un papel crucial en la forma en que nos comunicamos. Por otro lado, la comunicación verbal también es fundamental, ya que nos permite expresar nuestros pensamientos, sentimientos y emociones de manera clara y concisa.\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Vas a recibir un texto delimitado por triples comillas.\n",
        "Si contiene una secuencia de instrucciones, reescribe esas instrucciones en el siguiente formato:\n",
        "\n",
        "Paso 1 - ...\\n\n",
        "Paso 2 - ...\\n\n",
        "…\\n\n",
        "Paso N - ...\\n\n",
        "\n",
        "Si el texto no contiene una secuencia de instrucciones, entonces simplemente escribe \\\"No se proporcionaron pasos.\\\"\n",
        "\n",
        "Texto:\n",
        "```{text_1}```\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion(prompt)\n",
        "display(Markdown(f\"**Respuesta GPT:** {response}\"))\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "display(Markdown(f\"**Respuesta de LLAMA:** {preguntar_llama('Eres un asistente de IA cuyo trabajo es responder a lo que te está pidiendo el usuario de la forma más fiel al prompt', prompt=prompt)}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Tu tarea es responder de manera consistente.\n",
        "\n",
        "<hijo>: Enséñame sobre la paciencia.\n",
        "\n",
        "<abuelo>: El río que talla el valle más profundo fluye desde una modesta fuente; la sinfonía más grandiosa se origina a partir de una sola nota; el tapiz más intrincado comienza con un hilo solitario.\n",
        "\n",
        "<hijo>: Enséñame sobre la resiliencia.\n",
        "\"\"\"\n",
        "response = preguntar_gpt(prompt)\n",
        "display(Markdown(f\"**Respuesta GPT:** {response}\"))\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "display(Markdown(f\"**Respuesta de LLAMA:** {preguntar_llama('Eres un asistente de IA cuyo trabajo es responder a lo que te está pidiendo el usuario de la forma más fiel al prompt', prompt=prompt)}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Principio 2: Darle tiempo al modelo para \"pensar\" en la respuesta.\n",
        "\n",
        "Táctica: Especifica los pasos requeridos para completar la tarea."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "En un tranquilo bosque, los hermanos Mario y Sara se aventuraron en busca de un antiguo tesoro perdido. Mientras caminaban entre los árboles, riendo y compartiendo historias, un ruido extraño los detuvo de repente. MArio, intrigado, se acercó para investigar y accidentalmente activó una trampa oculta. Una red se desplegó atrapándolo, y Sara, al intentar liberarlo, cayó en un foso cercano.\n",
        "\n",
        "Afortunadamente, ninguno de los dos resultó gravemente herido. Juntos, encontraron la manera de salir de sus respectivas trampas y regresaron a casa con algunas raspaduras y moretones. Sin embargo, en lugar de desanimarse, se abrazaron con alivio y se rieron de su mala suerte.\n",
        "\n",
        "Decidieron que la próxima vez serían más cuidadosos, pero su determinación por descubrir el tesoro perdido no disminuyó. Con una nueva perspectiva sobre la aventura, continuaron explorando el bosque con entusiasmo renovado, emocionados por lo que el destino les depararía.\n",
        "\"\"\"\n",
        "\n",
        "prompt_1 = f\"\"\"\n",
        "Realiza las siguientes acciones: \\n\n",
        "1 - Resumir el siguiente texto delimitado por triple comillas con 1 frase.\\n\n",
        "2 - Traduzca el resumen al francés.\\n\n",
        "3 - Listar cada nombre en el resumen en francés.\\n\n",
        "4 - Generar un objeto json que contenga lo siguiente\\n\n",
        "claves: french_summary, num_names.\\n\n",
        "\n",
        "Separa tus respuestas con saltos de línea.\\n\n",
        "\n",
        "Texto:\\n\n",
        "```{text}```\\n\n",
        "\"\"\"\n",
        "response = preguntar_gpt(prompt_1)\n",
        "print(f\"**Respuesta GPT para el prompt 1:** {response}\")\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "print(f\"**Respuesta de LLAMA para el prompt 1:** {preguntar_llama('Eres un asistente de IA cuyo trabajo es responder a lo que te está pidiendo el usuario de la forma más fiel al prompt', prompt=prompt_1)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pide el resultado en un formato específico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_2 = f\"\"\"\n",
        "tu tarea consiste en realizar las siguientes acciones: \\n\n",
        "1 - Resumir el siguiente texto delimitado por <> con 1 frase.\\n\n",
        "2 - Traduzca el resumen al francés.\\n\n",
        "3 - Listar cada nombre en el resumen en francés.\\n\n",
        "4 - Dar salida a un objeto json que contenga las siguientes claves: french_summary, num_names.\\n\n",
        "\n",
        "Utilice el siguiente formato:\\n\n",
        "Texto: <texto a resumir>\\n\n",
        "Resumen: <resumen>\\n\n",
        "Traducción: <traducción del resumen>\\n\n",
        "Nombres: <lista de nombres en resumen>\\n\n",
        "JSON de salida: <json con resumen y número_nombres>\\n\n",
        "\n",
        "Texto: <{text}>\\n\n",
        "\"\"\"\n",
        "response = preguntar_gpt(prompt_2)\n",
        "print(f\"**Respuesta GPT para el prompt 2:** {response}\")\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "print(f\"**Respuesta de LLAMA para el prompt 2:** {preguntar_llama('Eres un asistente de IA cuyo trabajo es responder a lo que te está pidiendo el usuario de la forma más fiel al prompt', prompt=prompt_2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Táctica 2: Pedir al modelo que elabore su propia solución antes de llegar a una conclusión precipitada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Limitaciones del modelo: Alucinaciones\n",
        "\n",
        "Apple es una compañía de tecnología real pero iCan no es un producto real de Apple. Sin embargo, el modelo puede alucinar que iCan es un producto real de Apple y devolvernos una respuesta que no es cierta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Háblame sobre el iCan, la última innovación en tecnología de dispositivos portátiles de Apple.\n",
        "\"\"\"\n",
        "response = preguntar_gpt(prompt)\n",
        "display(Markdown(f\"**Respuesta GPT:** {response}\"))\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "display(Markdown(f\"**Respuesta de LLAMA:** {preguntar_llama('Eres un asistente de IA cuyo trabajo es responder a lo que te está pidiendo el usuario de la forma más fiel al prompt', prompt=prompt)}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tu turno de experimentar!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "3.11.4",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
