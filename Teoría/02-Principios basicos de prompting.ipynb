{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Principios básicos del prompting\n",
        "\n",
        "1. Los prompts deben ser sencillos y claros. No es necesario escribir un párrafo entero para obtener una respuesta.\n",
        "2. Dale al modelo tiempo para \"pensar\" en la respuesta. No le des un prompt y esperes una respuesta inmediata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "import sys\n",
        "\n",
        "from utils.funciones import preguntar_llama, preguntar_gpt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tácticas\n",
        "\n",
        "1. Utiliza delimitadores para indicar cómo distinguir partes del input.\n",
        "    Delimitadores pueden ser ```, \"\"\", < >, `<tag> </tag>`, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Respuesta GPT:** A menor temperatura en un LLM, los resultados son más deterministas y certeros, mientras que un aumento en la temperatura promueve respuestas más variadas y creativas, aunque puede generar alucinaciones."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Respuesta de LLAMA:** Un Modelo de Lenguaje de Gran Escala tiende a ser más determinista a menor temperatura y más variado y creativo a mayor temperatura."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Definimos el texto\n",
        "text = \"\"\"En esencia, a menor temperatura en un Modelo de Lenguaje de Gran Escala (LLM), los resultados tienden a ser más deterministas. Esto implica que el modelo optará con más frecuencia por el siguiente token de mayor probabilidad. Incrementar la temperatura introduce mayor variabilidad en las respuestas, lo que promueve resultados más variados o creativos. Básicamente, al elevar la temperatura, se incrementa la posibilidad de elegir diferentes tokens como opciones. Desde un punto de vista práctico, en tareas como preguntas y respuestas basadas en información objetiva, se sugiere utilizar un valor de temperatura reducido para lograr respuestas más certeras y breves. Sin embargo, en la creación de poesía o en otras actividades creativas, puede resultar ventajoso aumentar el valor de la temperatura. El aumento en la temperatura puede generar alucinaciones.\"\"\"\n",
        "\n",
        "# Definimos el prompt\n",
        "prompt = f\"Resume el texto delimitado por las triples comillas en una sola oración: {text}\"\n",
        "\n",
        "# Obtenemos la respuesta\n",
        "response = preguntar_gpt(prompt)\n",
        "display(Markdown(f\"**Respuesta GPT:** {response}\"))\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "display(Markdown(f\"**Respuesta de LLAMA:** {preguntar_llama(prompt=prompt)}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. JSON, HTML, XML, YAML, etc. son formatos que el modelo puede entender y que pueden ayudar a estructurar el input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Respuesta GPT:** Aquí tienes una lista de tres títulos de libros inventados en formato JSON:\n",
            "\n",
            "```json\n",
            "[\n",
            "    {\n",
            "        \"book_id\": 1,\n",
            "        \"título\": \"El susurro del tiempo\",\n",
            "        \"autor\": \"María Fernández\",\n",
            "        \"género\": \"Fantasía\"\n",
            "    },\n",
            "    {\n",
            "        \"book_id\": 2,\n",
            "        \"título\": \"Caminos de acero\",\n",
            "        \"autor\": \"Juan Pérez\",\n",
            "        \"género\": \"Ciencia ficción\"\n",
            "    },\n",
            "    {\n",
            "        \"book_id\": 3,\n",
            "        \"título\": \"Bajo el cielo de París\",\n",
            "        \"autor\": \"Sofía Martínez\",\n",
            "        \"género\": \"Romance\"\n",
            "    }\n",
            "]\n",
            "```\n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "**Respuesta de LLAMA:** Claro, aquí te dejo la lista que solicitaste:\n",
            "\n",
            "```json\n",
            "[\n",
            "  {\n",
            "    \"book_id\": 1,\n",
            "    \"título\": \"La ciudad oculta\",\n",
            "    \"autor\": \"Aurora Díaz\",\n",
            "    \"género\": \"Ciencia ficción\"\n",
            "  },\n",
            "  {\n",
            "    \"book_id\": 2,\n",
            "    \"título\": \"El secreto de la biblioteca maldita\",\n",
            "    \"autor\": \"Julio Mármol\",\n",
            "    \"género\": \"Misterio\"\n",
            "  },\n",
            "  {\n",
            "    \"book_id\": 3,\n",
            "    \"título\": \"En el corazón del tiempo\",\n",
            "    \"autor\": \"Elsa Fuenmayor\",\n",
            "    \"género\": \"Romántico histórico\"\n",
            "  }\n",
            "]\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Genera una lista de tres títulos de libros inventados junto con sus autores y géneros. Proporciónelos en formato JSON con las siguientes claves: book_id, título, autor, género\"\"\"\n",
        "response = preguntar_gpt(prompt)\n",
        "\n",
        "print(f\"**Respuesta GPT:** {response}\")\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "print(f\"**Respuesta de LLAMA:** {preguntar_llama(prompt=prompt)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Pídele al modelo que verifique si se están cumpliendo ciertas condiciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Respuesta GPT:** No se proporcionaron pasos."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Respuesta de LLAMA:** No se proporcionaron pasos.\n",
              "\n",
              "El texto es una descripción del proceso para preparar un café en una máquina espresso, pero no contiene una secuencia de instrucciones numéricas que puedan ser reescritas en el formato solicitado."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "text_1 = \"\"\"Preparar una exquisita taza de café en una máquina espresso es un proceso sencillo. En primer lugar, asegúrate de que la máquina esté encendida y lista para usar. Luego, muele los granos de café según el grosor deseado y colócalos en el portafiltro. Compacta el café con el tampador para garantizar una extracción uniforme. Ajusta la cantidad de café según tu preferencia y coloca el portafiltro en la máquina. Activa la máquina y espera a que el café se extraiga lentamente, disfrutando del aroma que llena el aire. ¡Y voilà! Disfruta de tu aromático y delicioso café espresso.\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Vas a recibir un texto delimitado por triples comillas.\n",
        "Si contiene una secuencia de instrucciones, reescribe esas instrucciones en el siguiente formato:\n",
        "\n",
        "Paso 1 - ...\\n\n",
        "Paso 2 - ...\\n\n",
        "…\\n\n",
        "Paso N - ...\\n\n",
        "\n",
        "Si el texto no contiene una secuencia de instrucciones, entonces simplemente escribe \\\"No se proporcionaron pasos.\\\"\n",
        "\n",
        "Texto:\n",
        "```{text_1}```\n",
        "\"\"\"\n",
        "\n",
        "response = preguntar_gpt(prompt)\n",
        "display(Markdown(f\"**Respuesta GPT:** {response}\"))\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "display(Markdown(f\"**Respuesta de LLAMA:** {preguntar_llama(prompt=prompt)}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Few-shot prompting: Utiliza ejemplos para que el modelo entienda mejor lo que quieres."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Respuesta GPT:** No se proporcionaron pasos."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Respuesta de LLAMA:** No se proporcionaron pasos."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "text_1 = \"\"\"Los humanos son seres sociales por naturaleza y, por lo tanto, la comunicación es una parte esencial de la vida cotidiana. La comunicación efectiva puede ser verbal o no verbal, y ambas formas son igualmente importantes. Algunos estudios sugieren que el 93% de la comunicación es no verbal, lo que significa que el lenguaje corporal, las expresiones faciales y otros gestos juegan un papel crucial en la forma en que nos comunicamos. Por otro lado, la comunicación verbal también es fundamental, ya que nos permite expresar nuestros pensamientos, sentimientos y emociones de manera clara y concisa.\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Vas a recibir un texto delimitado por triples comillas.\n",
        "Si contiene una secuencia de instrucciones, reescribe esas instrucciones en el siguiente formato:\n",
        "\n",
        "Paso 1 - ...\\n\n",
        "Paso 2 - ...\\n\n",
        "…\\n\n",
        "Paso N - ...\\n\n",
        "\n",
        "Si el texto no contiene una secuencia de instrucciones, entonces simplemente escribe \\\"No se proporcionaron pasos.\\\"\n",
        "\n",
        "Texto:\n",
        "```{text_1}```\n",
        "\"\"\"\n",
        "\n",
        "response = preguntar_gpt(prompt)\n",
        "display(Markdown(f\"**Respuesta GPT:** {response}\"))\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "display(Markdown(f\"**Respuesta de LLAMA:** {preguntar_llama(prompt=prompt)}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Respuesta GPT:** No se proporcionaron pasos."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Respuesta de LLAMA:** No se proporcionaron pasos."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "text_1 = \"\"\"Los humanos son seres sociales por naturaleza y, por lo tanto, la comunicación es una parte esencial de la vida cotidiana. La comunicación efectiva puede ser verbal o no verbal, y ambas formas son igualmente importantes. Algunos estudios sugieren que el 93% de la comunicación es no verbal, lo que significa que el lenguaje corporal, las expresiones faciales y otros gestos juegan un papel crucial en la forma en que nos comunicamos. Por otro lado, la comunicación verbal también es fundamental, ya que nos permite expresar nuestros pensamientos, sentimientos y emociones de manera clara y concisa.\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Vas a recibir un texto delimitado por triples comillas.\n",
        "Si contiene una secuencia de instrucciones, reescribe esas instrucciones en el siguiente formato:\n",
        "\n",
        "Paso 1 - ...\\n\n",
        "Paso 2 - ...\\n\n",
        "…\\n\n",
        "Paso N - ...\\n\n",
        "\n",
        "Si el texto no contiene una secuencia de instrucciones, entonces simplemente escribe \\\"No se proporcionaron pasos.\\\"\n",
        "\n",
        "Texto:\n",
        "```{text_1}```\n",
        "\"\"\"\n",
        "\n",
        "response = preguntar_gpt(prompt)\n",
        "display(Markdown(f\"**Respuesta GPT:** {response}\"))\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "display(Markdown(f\"**Respuesta de LLAMA:** {preguntar_llama(prompt=prompt)}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Respuesta GPT:** <abuelo>: La caña que se dobla ante el viento no se quiebra, mientras que el roble que se aferra a su posición puede caer. Las semillas que enfrentan la tormenta encuentran su camino hacia la luz, y cada cicatriz es un recordatorio de la fuerza que llevamos dentro."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Respuesta de LLAMA:** <abuelo>: La fuerza del viento que despeña las hojas de los árboles se alimenta de cada tormenta que ha pasado, y el canto del pájaro más tenue se refuerza con cada ruego que es lanzado. Al igual que un granizo que cae en una fuente, la resiliencia comienza con pequeñas acciones cotidianas y, con el tiempo, puede volverse tan profunda como el abismo y tan hermosa como una melodía."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "Tu tarea es responder de manera consistente.\n",
        "\n",
        "<hijo>: Enséñame sobre la paciencia.\n",
        "\n",
        "<abuelo>: El río que talla el valle más profundo fluye desde una modesta fuente; la sinfonía más grandiosa se origina a partir de una sola nota; el tapiz más intrincado comienza con un hilo solitario.\n",
        "\n",
        "<hijo>: Enséñame sobre la resiliencia.\n",
        "\"\"\"\n",
        "response = preguntar_gpt(prompt)\n",
        "display(Markdown(f\"**Respuesta GPT:** {response}\"))\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "display(Markdown(f\"**Respuesta de LLAMA:** {preguntar_llama(prompt=prompt)}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Principio 2: Darle tiempo al modelo para \"pensar\" en la respuesta.\n",
        "\n",
        "Táctica: Especifica los pasos requeridos para completar la tarea."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Respuesta GPT para el prompt 1:** 1 - Mario y Sara, dos hermanos, se aventuran en un bosque en busca de un tesoro perdido, enfrentándose a trampas pero regresando a casa con una nueva determinación.\n",
            "\n",
            "2 - Mario et Sara, deux frères, s'aventurent dans une forêt à la recherche d'un trésor perdu, faisant face à des pièges mais rentrant chez eux avec une nouvelle détermination.\n",
            "\n",
            "3 - Mario, Sara\n",
            "\n",
            "4 - \n",
            "```json\n",
            "{\n",
            "  \"french_summary\": \"Mario et Sara, deux frères, s'aventurent dans une forêt à la recherche d'un trésor perdu, faisant face à des pièges mais rentrant chez eux avec une nouvelle détermination.\",\n",
            "  \"num_names\": 2\n",
            "}\n",
            "```\n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "**Respuesta de LLAMA para el prompt 1:** 1 - Resumir el texto delimitado por triple comillas en 1 frase:\n",
            "\n",
            "\"Los hermanos Mario y Sara se aventuraron en busca de un tesoro perdido en un bosque tranquilo, pero su emoción no disminuyó después de enfrentar una trampa oculta.\"\n",
            "\n",
            "2 - Traducir el resumen al francés:\n",
            "\n",
            "\n",
            "\"Les frères Mario et Sara s'aventurèrent en quête d'un trésor perdu dans une forêt paisible, mais leur émotion ne diminua pas après avoir rencontré un piège caché.\"\n",
            "\n",
            "\n",
            "3 - Listar cada nombre mencionado en el resumen:\n",
            "\n",
            "\n",
            "* Mario\n",
            "* Sara\n",
            "\n",
            "\n",
            "4 - Generar un objeto json con la información solicitada:\n",
            "\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"french_summary\": \"Les frères Mario et Sara s'aventurèrent en quête d'un trésor perdu dans une forêt paisible, mais leur émotion ne diminua pas après avoir rencontré un piège caché.\",\n",
            "  \"num_names\": 2\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"\n",
        "En un tranquilo bosque, los hermanos Mario y Sara se aventuraron en busca de un antiguo tesoro perdido. Mientras caminaban entre los árboles, riendo y compartiendo historias, un ruido extraño los detuvo de repente. MArio, intrigado, se acercó para investigar y accidentalmente activó una trampa oculta. Una red se desplegó atrapándolo, y Sara, al intentar liberarlo, cayó en un foso cercano.\n",
        "\n",
        "Afortunadamente, ninguno de los dos resultó gravemente herido. Juntos, encontraron la manera de salir de sus respectivas trampas y regresaron a casa con algunas raspaduras y moretones. Sin embargo, en lugar de desanimarse, se abrazaron con alivio y se rieron de su mala suerte.\n",
        "\n",
        "Decidieron que la próxima vez serían más cuidadosos, pero su determinación por descubrir el tesoro perdido no disminuyó. Con una nueva perspectiva sobre la aventura, continuaron explorando el bosque con entusiasmo renovado, emocionados por lo que el destino les depararía.\n",
        "\"\"\"\n",
        "\n",
        "prompt_1 = f\"\"\"\n",
        "Realiza las siguientes acciones: \\n\n",
        "1 - Resumir el siguiente texto delimitado por triple comillas con 1 frase.\\n\n",
        "2 - Traduzca el resumen al francés.\\n\n",
        "3 - Listar cada nombre en el resumen en francés.\\n\n",
        "4 - Generar un objeto json que contenga lo siguiente\\n\n",
        "claves: french_summary, num_names.\\n\n",
        "\n",
        "Separa tus respuestas con saltos de línea.\\n\n",
        "\n",
        "Texto:\\n\n",
        "```{text}```\\n\n",
        "\"\"\"\n",
        "response = preguntar_gpt(prompt_1)\n",
        "print(f\"**Respuesta GPT para el prompt 1:** {response}\")\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "print(f\"**Respuesta de LLAMA para el prompt 1:** {preguntar_llama(prompt=prompt_1)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pide el resultado en un formato específico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Respuesta GPT para el prompt 2:** Resumen: Los hermanos Mario y Sara buscan un tesoro en el bosque, pero activan trampas que los atrapan, aunque logran salir y continúan su aventura con determinación.\n",
            "\n",
            "Traducción: Les frères Mario et Sara cherchent un trésor dans la forêt, mais ils activent des pièges qui les capturent, bien qu'ils parviennent à s'échapper et continuent leur aventure avec détermination.\n",
            "\n",
            "Nombres: Mario, Sara\n",
            "\n",
            "JSON de salida: {\"french_summary\": \"Les frères Mario et Sara cherchent un trésor dans la forêt, mais ils activent des pièges qui les capturent, bien qu'ils parviennent à s'échapper et continuent leur aventure avec détermination.\", \"num_names\": 2}\n",
            "\n",
            "---------------------------------------------\n",
            "\n",
            "**Respuesta de LLAMA para el prompt 2:** Texto: <Texto delimitado anteriormente>\n",
            "\n",
            "Resumen: Los hermanos Mario y Sara se aventuraron en busca de un antiguo tesoro perdido.\n",
            "\n",
            "Traducción: Les frères Mario et Sara sont allés en quête d'un trésor perdu.\n",
            "\n",
            "Nombres: -Mario, -Sara\n",
            "\n",
            "JSON de salida:\n",
            "{\n",
            "\"french_summary\": \"Les frères Mario et Sara sont allés en quête d'un trésor perdu.\",\n",
            "\"num_names\": 2\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "prompt_2 = f\"\"\"\n",
        "tu tarea consiste en realizar las siguientes acciones: \\n\n",
        "1 - Resumir el siguiente texto delimitado por <> con 1 frase.\\n\n",
        "2 - Traduzca el resumen al francés.\\n\n",
        "3 - Listar cada nombre en el resumen en francés.\\n\n",
        "4 - Dar salida a un objeto json que contenga las siguientes claves: french_summary, num_names.\\n\n",
        "\n",
        "Utilice el siguiente formato:\\n\n",
        "Texto: <texto a resumir>\\n\n",
        "Resumen: <resumen>\\n\n",
        "Traducción: <traducción del resumen>\\n\n",
        "Nombres: <lista de nombres en resumen>\\n\n",
        "JSON de salida: <json con resumen y número_nombres>\\n\n",
        "\n",
        "Texto: <{text}>\\n\n",
        "\"\"\"\n",
        "response = preguntar_gpt(prompt_2)\n",
        "print(f\"**Respuesta GPT para el prompt 2:** {response}\")\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "print(f\"**Respuesta de LLAMA para el prompt 2:** {preguntar_llama(prompt=prompt_2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Táctica 2: Pedir al modelo que elabore su propia solución antes de llegar a una conclusión precipitada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Limitaciones del modelo: Alucinaciones\n",
        "\n",
        "Apple es una compañía de tecnología real pero iCan no es un producto real de Apple. Sin embargo, el modelo puede alucinar que iCan es un producto real de Apple y devolvernos una respuesta que no es cierta.\n",
        "\n",
        "Los modelos cada vez responden mejor evitando las alucinaciones, esta pregunta el año pasado hubiera sido un ejemplo perfecto de alucinación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Respuesta GPT:** Hasta la fecha de mi último entrenamiento en octubre de 2023, no hay información sobre un dispositivo llamado \"iCan\" de Apple. Es posible que te refieras a un nuevo producto que ha sido lanzado o anunciado después de esa fecha, o que el nombre no sea oficial. Apple ha estado trabajando en varias innovaciones en el ámbito de dispositivos portátiles, como el Apple Watch, que ha evolucionado con nuevas funciones de salud y conectividad, así como en otros dispositivos como AirPods y dispositivos de realidad aumentada.\n",
              "\n",
              "Si \"iCan\" es una nueva innovación, te recomendaría consultar el sitio web oficial de Apple o fuentes de noticias tecnológicas actualizadas para obtener información precisa y reciente. Si tienes más detalles sobre el dispositivo o su propósito, estaré encantado de ayudarte con la información que tengo."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Respuesta de LLAMA:** Lo siento, pero no tengo información actualizada sobre un dispositivo llamado \"iCan\" de Apple. Sin embargo, puedo intentar ayudarte a encontrar lo que busques.\n",
              "\n",
              "Es posible que estés confundiendo el iCan con otro producto o característica de Apple. Una vez más en los últimos años, Apple ha lanzado varios dispositivos innovadores como la Mac mini M1, el iPad Pro y otros, pero sin embargo no encuentre información específica sobre un \"iCan\".\n",
              "\n",
              "Si lo que buscas es una guía general sobre los productos portátiles de Apple o tienes alguna otra pregunta acerca del iMac, el MacBook, AirPods o otro dispositivo portátil de Apple, estaré aquí para ayudarte.\n",
              "\n",
              "¿Hay algo en particular que estés buscando saber o alguna otra forma en cómo pueda ser más útil?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "Háblame sobre el iCan, la última innovación en tecnología de dispositivos portátiles de Apple.\n",
        "\"\"\"\n",
        "response = preguntar_gpt(prompt)\n",
        "display(Markdown(f\"**Respuesta GPT:** {response}\"))\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "display(Markdown(f\"**Respuesta de LLAMA:** {preguntar_llama(prompt=prompt)}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Respuesta GPT:** 2332 multiplicado por 9450 es igual a 22,046,400."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Respuesta de LLAMA:** La respuesta es: 21,981,900."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Solución real para la multiplicación: 2203740\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "Multiplica 2332 por 9450.\n",
        "\"\"\"\n",
        "# Los modelos cada vez son más inteligentes, pero aún no son capaces de hacer cálculos matemáticos complejos.\n",
        "\n",
        "response = preguntar_gpt(prompt)\n",
        "display(Markdown(f\"**Respuesta GPT:** {response}\"))\n",
        "print(\"\\n---------------------------------------------\\n\")\n",
        "display(Markdown(f\"**Respuesta de LLAMA:** {preguntar_llama(prompt=prompt)}\"))\n",
        "print(f\"Solución real para la multiplicación: {2332*945}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tu turno de experimentar!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
