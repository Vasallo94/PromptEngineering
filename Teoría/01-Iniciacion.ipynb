{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iniciación al Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los Modelos de Lenguaje de Gran Escala (LLM) son sistemas de inteligencia artificial diseñados para generar texto humano coherente y contextualmente relevante. Para generar el siguiente token en una secuencia, los LLM utilizan una serie de parámetros que influyen en la selección del token adecuado. Conocer y ajustar estos parámetros es crucial para obtener resultados óptimos y adaptados a diferentes contextos y tareas.\n",
    "\n",
    "Algunos de los parámetros clave en la generación de tokens incluyen la temperatura, que determina el grado de aleatoriedad en la selección de tokens, y el Top P, que controla la probabilidad acumulada de los tokens considerados. Además, la longitud máxima y las secuencias de parada ayudan a controlar la extensión y estructura de las respuestas generadas. También existen penalizaciones por frecuencia y presencia para evitar la repetición innecesaria de palabras o frases.\n",
    "\n",
    "Entender y ajustar estos parámetros permite a los usuarios adaptar el comportamiento del LLM a diferentes aplicaciones, como responder preguntas basadas en hechos, generar contenido creativo o realizar traducciones automáticas, entre otras. Por lo tanto, es esencial familiarizarse con estos parámetros para aprovechar al máximo las capacidades de un LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El papel de los parámetros en los LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Temperatura (Temperature):** En esencia, a menor temperatura en un Modelo de Lenguaje de Gran Escala (LLM), los resultados tienden a ser más deterministas. Esto implica que el modelo optará con más frecuencia por el siguiente token de mayor probabilidad. Incrementar la temperatura introduce mayor variabilidad en las respuestas, lo que promueve resultados más variados o creativos. Básicamente, al elevar la temperatura, se incrementa la posibilidad de elegir diferentes tokens como opciones. Desde un punto de vista práctico, en tareas como preguntas y respuestas basadas en información objetiva, se sugiere utilizar un valor de temperatura reducido para lograr respuestas más certeras y breves. Sin embargo, en la creación de poesía o en otras actividades creativas, puede resultar ventajoso aumentar el valor de la temperatura. El aumento en la temperatura puede generar alucinaciones.\n",
    "\n",
    "**Top P**: Esta es una técnica de muestreo en conjunto con la temperatura, conocida como muestreo de núcleo, que te permite controlar cuán determinista es el modelo. Si buscas respuestas exactas y basadas en hechos, es aconsejable mantener un valor bajo de Top P. Si, por el contrario, deseas respuestas más variadas, puedes aumentar este valor. Utilizar Top P significa que solo se consideran para las respuestas los tokens que constituyen la masa de probabilidad top_p, por lo que un valor bajo de top_p seleccionará las respuestas más seguras. Un valor alto de top_p, en cambio, permitirá que el modelo considere un rango más amplio de palabras posibles, incluyendo aquellas menos probables, lo que conduce a resultados más variados. La recomendación general es modificar la temperatura o el Top P, pero no ambos al mismo tiempo.\n",
    "\n",
    "El **Top K** es otro parámetro importante en la generación de texto por parte de los Modelos de Lenguaje de Gran Escala (LLM). Este parámetro se refiere a la selección de los K tokens más probables como candidatos para el siguiente token en la secuencia. Al ajustar el valor de K, los usuarios pueden controlar el grado de determinismo en las respuestas generadas por el modelo. Un valor de K más pequeño resultará en respuestas más deterministas y enfocadas, mientras que un valor de K más grande permitirá una mayor diversidad y creatividad en las respuestas al considerar un rango más amplio de tokens. \n",
    "\n",
    "La diferencia principal entre el Top P y el Top K radica en la forma en que ambos parámetros restringen el conjunto de tokens considerados para la generación del siguiente token en un Modelo de Lenguaje de Gran Escala (LLM).\n",
    "\n",
    "Top P se basa en la probabilidad acumulada de los tokens. En lugar de seleccionar un número fijo de tokens más probables (como hace el Top K), el Top P considera todos los tokens cuya probabilidad acumulada sea igual o inferior al valor de P establecido. Esto significa que el número de tokens considerados puede variar, dependiendo de las probabilidades asignadas a cada token por el modelo.\n",
    "\n",
    "Por otro lado, el Top K selecciona un número fijo de tokens más probables (K) como candidatos para el siguiente token. Al ajustar el valor de K, se limita el conjunto de tokens candidatos a los K tokens con mayor probabilidad.\n",
    "\n",
    "**Longitud Máxima(Max Length)**: Puedes gestionar el número de tokens que el modelo genera ajustando la longitud máxima. Especificar una longitud máxima te ayuda a evitar respuestas demasiado largas o irrelevantes y a controlar los costos asociados.\n",
    "\n",
    "**Secuencias de Parada(Stop Sequences)**: Una secuencia de parada es una cadena de texto que detiene la generación de tokens por parte del modelo. Especificar secuencias de parada es otra manera de controlar la longitud y estructura de las respuestas del modelo. Por ejemplo, puedes indicar al modelo que genere listas que no tengan más de 10 elementos añadiendo \"11\" como secuencia de parada.\n",
    "\n",
    "**Penalización por Frecuencia(Frequency Penalty)**: Esta penalización se aplica al siguiente token proporcionalmente a la cantidad de veces que ese token ya ha aparecido en la respuesta y en la indicación. Cuanto mayor sea la penalización por frecuencia, menos probable será que una palabra aparezca de nuevo. Esta configuración reduce la repetición de palabras en la respuesta del modelo al aplicar una penalización mayor a los tokens que aparecen con más frecuencia.\n",
    "\n",
    "**Penalización por Presencia(Presence Penalty)**: Al igual que la penalización por frecuencia, la penalización por presencia se aplica a los tokens repetidos, pero a diferencia de la primera, la penalización es la misma para todos los tokens repetidos, independientemente de la frecuencia de su aparición. Esto evita que el modelo repita frases con demasiada frecuencia en su respuesta. Si deseas que el modelo genere texto diverso o creativo, podrías optar por usar una penalización por presencia más alta. Por otro lado, si necesitas que el modelo mantenga el enfoque, podrías probar con una penalización por presencia más baja.\n",
    "\n",
    "Al igual que con la temperatura y el Top P, la recomendación general es modificar la penalización por frecuencia o por presencia, pero no ambas al mismo tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup del entorno y de la apikey de OpenAI y LLMCLoud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "import requests\n",
    "import os \n",
    "from IPython.display import Markdown, display\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(os.path.join(\"..\", \".env\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante todo el curso vamos a utilizar tanto GPT-4o-mini de OpenAI (de pago), como LLAMA (gratuito). Para poder utilizarlos necesitamos una apikey que se obtiene a través de la web de [OpenAI](https://platform.openai.com/settings/profile?tab=api-keys) y la web [LLMCloud](https://www.llmcloud.app/keys) y que vamos a guardar en un archivo ```.env```. Para ello, vamos a instalar la librería python-dotenv que nos permitirá cargar las variables de entorno desde un archivo ```.env```. Dentro de ese archivo vamos a guardar tanto la apikey de OpenAI como la de LLMCloud de esta forma: \n",
    "```\n",
    "OPENAI_API_KEY = tu_apikey\n",
    "```\n",
    "```\n",
    "AUTH_TOKEN = tu_apikey\n",
    "```\n",
    "\n",
    "Vamos a crear también unas funciones que nos ayuden a la hora de usar los prompts para que sea más sencillo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función mejorada para consultar la API de LLMCloud utilizando el modelo \"Meta-Llama-3-8B-Instruct\".\n",
    "# Esta función envía una solicitud POST con un mensaje de configuración (sys_prompt) y un mensaje de usuario (prompt).\n",
    "# Se espera que la respuesta JSON contenga una lista en 'choices' y se obtiene el contenido del primer mensaje generado.\n",
    "\n",
    "# Se utiliza el token de autenticación obtenido de las variables de entorno 'AUTH_TOKEN'\n",
    "auth_token = os.getenv(\"AUTH_TOKEN\")\n",
    "\n",
    "# URL y cabeceras para la petición a la API\n",
    "url = \"https://api.awanllm.com/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {auth_token}\"\n",
    "}\n",
    "\n",
    "def preguntar_llama(\n",
    "    prompt,\n",
    "    sys_prompt=\"Eres un asistente útil\",\n",
    "    max_retries=3,\n",
    "    initial_delay=10,\n",
    "    timeout=60,\n",
    "    rate_limit_delay=5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Sends a prompt to an AI model and retrieves the response.\n",
    "\n",
    "    Args:\n",
    "        sys_prompt (str): The system prompt to be sent to the AI model.\n",
    "        prompt (str): The user prompt to be sent to the AI model.\n",
    "        max_retries (int, optional): The maximum number of retries in case of errors. Defaults to 3.\n",
    "        initial_delay (int, optional): The initial delay between retries in seconds. Defaults to 1.\n",
    "        timeout (int, optional): The timeout for the HTTP request in seconds. Defaults to 60.\n",
    "        rate_limit_delay (int, optional): The delay between retries in case of rate limiting errors. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        str: The response generated by the AI model.\n",
    "\n",
    "    Raises:\n",
    "        requests.exceptions.HTTPError: If an HTTP error occurs during the request.\n",
    "        requests.exceptions.Timeout: If the request times out.\n",
    "        Exception: If an unexpected error occurs.\n",
    "\n",
    "    \"\"\"\n",
    "    payload = json.dumps(\n",
    "        {\n",
    "            \"model\": \"Meta-Llama-3-8B-Instruct\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                url, headers=headers, data=payload, timeout=timeout\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            json_response = response.json()\n",
    "            return json_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            status_code = e.response.status_code\n",
    "            if status_code == 429:\n",
    "                print(\n",
    "                    f\"Intento {attempt + 1} fallido. Error: {e}. Demasiadas solicitudes.\"\n",
    "                )\n",
    "                delay = rate_limit_delay\n",
    "            elif status_code == 502:\n",
    "                print(f\"Intento {attempt + 1} fallido. Error: {e}. Error de Gateway.\")\n",
    "                delay = initial_delay\n",
    "            else:\n",
    "                print(f\"Intento {attempt + 1} fallido. Error: {e}\")\n",
    "                break  # Salir del bucle si hay un error HTTP que no sea 429 o 502\n",
    "\n",
    "            print(f\"Reintentando en {delay} segundos...\")\n",
    "            time.sleep(delay)\n",
    "        except requests.exceptions.Timeout as e:\n",
    "            print(f\"Intento {attempt + 1} fallido por tiempo de espera excedido: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(\"Reintentando...\")\n",
    "            time.sleep(initial_delay * (2**attempt))\n",
    "        except Exception as e:\n",
    "            print(f\"Error no esperado: {e}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                return None  # Retornar None solo después de todos los intentos\n",
    "\n",
    "    print(\"No se pudo completar la solicitud después de varios intentos.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Puedo ayudarte con una amplia variedad de tareas, como:\n",
       "\n",
       "*   Respondiendo a preguntas: puedo proporcionar información en muchos diferentes temas y campos.\n",
       "*   Generando textos: puedes pedirme que te genere un texto sobre cualquier tema, desde la escritura creativa hasta el redactar documentos técnicos o simplemente para explicarte cómo se hace algo. \n",
       "*   Realizar tareas matemáticas: puedo resolver problemas matemáticos complejos, desde aritmética básica hasta ecuaciones integrales y más allá. Además, puedo ofrecer ayuda con estadísticas, graficar gráficas y analizar datos.\n",
       "*   Ayudando a planificar: Puedo ayudarte a crear planes o guías paso a paso para proyectos grandes o pequeños, lo cual puede ser muy útil si tienes algo que quieres lograr pero no sabes por dónde empezar. \n",
       "*   Conversación y comprensión del lenguaje natural: Puedo entablar una conversación con personas sobre cualquier tema imaginable (dentro de los límites dados por la base de datos), e incluso puedo responder preguntas o completar frases.\n",
       "*   Traducción de texto: Si tienes algo que necesitas traducir, como un mensaje corto, un libro largo, un documento para un negocio o más, te puedo ayudar a encontrar una forma rápida y eficiente."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(preguntar_llama(sys_prompt=\"Eres un asistente de IA útil.\", prompt='¿Cuál es tu función?')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Respuesta:** ¡Hola! Puedo ayudarte con una variedad de cosas, como responder preguntas, proporcionarte información sobre temas específicos, ayudarte a resolver problemas, ofrecerte consejos, sugerir ideas, y mucho más. ¿En qué te gustaría que te ayude hoy?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# Inicializamos el cliente de OpenAI utilizando la API Key almacenada en la variable de entorno \"OPENAI_API_KEY\"\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Prompt del usuario: mensaje que se enviará a la API\n",
    "prompt = \"Hola, qué puedes hacer por mí?\"\n",
    "\n",
    "def preguntar_gpt(\n",
    "    prompt,\n",
    "    sys_prompt=\"Eres un asistente útil.\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    max_retries=3,\n",
    "    timeout=60,\n",
    "    rate_limit_delay=5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Envía una solicitud a la API de OpenAI usando el modelo y parámetros indicados.\n",
    "    \n",
    "    Parámetros:\n",
    "      prompt (str): Mensaje del usuario.\n",
    "      sys_prompt (str): Mensaje de sistema que define el comportamiento del modelo.\n",
    "      model (str): Modelo de OpenAI a utilizar (ej. \"gpt-4o-mini\").\n",
    "      temperature (float): Controla la aleatoriedad en la generación del texto.\n",
    "      max_retries (int): Número máximo de reintentos en caso de error.\n",
    "      timeout (int): Tiempo máximo en segundos para esperar la respuesta.\n",
    "      rate_limit_delay (int): Tiempo de espera en segundos al alcanzar el límite de solicitudes.\n",
    "    \n",
    "    Retorna:\n",
    "      str o None: Respuesta generada por la API o None si se agotan los intentos.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                timeout=timeout,\n",
    "            )\n",
    "            # Se retorna el contenido del primer mensaje de la respuesta.\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Intento {attempt + 1} fallido: {e}. Reintentando en {rate_limit_delay} segundos...\"\n",
    "            )\n",
    "            time.sleep(rate_limit_delay)\n",
    "\n",
    "    print(\"No se pudo obtener una respuesta tras varios intentos.\")\n",
    "    return None\n",
    "\n",
    "# Solicitamos la respuesta a la API con un sys_prompt más descriptivo\n",
    "response = preguntar_gpt(\n",
    "    prompt=prompt,\n",
    "    sys_prompt=(\"Eres un asistente de IA cuyo trabajo es responder a lo que se te está pidiendo de la forma más fiel al prompt\")\n",
    ")\n",
    "\n",
    "display(Markdown(f\"**Respuesta:** {response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez ya tenemos las funciones que nos permiten hacer peticiones contra LLAMA y GPT-4o-mini vamos a extraerlas a un archivo llamado ```utils/funciones.py``` para que sea más sencillo su uso en los siguientes notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
