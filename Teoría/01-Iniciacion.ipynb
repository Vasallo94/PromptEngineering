{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iniciación al Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los Modelos de Lenguaje de Gran Escala (LLM) son sistemas de inteligencia artificial diseñados para generar texto humano coherente y contextualmente relevante. Para generar el siguiente token en una secuencia, los LLM utilizan una serie de parámetros que influyen en la selección del token adecuado. Conocer y ajustar estos parámetros es crucial para obtener resultados óptimos y adaptados a diferentes contextos y tareas.\n",
    "\n",
    "Algunos de los parámetros clave en la generación de tokens incluyen la temperatura, que determina el grado de aleatoriedad en la selección de tokens, y el Top P, que controla la probabilidad acumulada de los tokens considerados. Además, la longitud máxima y las secuencias de parada ayudan a controlar la extensión y estructura de las respuestas generadas. También existen penalizaciones por frecuencia y presencia para evitar la repetición innecesaria de palabras o frases.\n",
    "\n",
    "Entender y ajustar estos parámetros permite a los usuarios adaptar el comportamiento del LLM a diferentes aplicaciones, como responder preguntas basadas en hechos, generar contenido creativo o realizar traducciones automáticas, entre otras. Por lo tanto, es esencial familiarizarse con estos parámetros para aprovechar al máximo las capacidades de un LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## El papel de los parámetros en los LLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Temperatura (Temperature):** En esencia, a menor temperatura en un Modelo de Lenguaje de Gran Escala (LLM), los resultados tienden a ser más deterministas. Esto implica que el modelo optará con más frecuencia por el siguiente token de mayor probabilidad. Incrementar la temperatura introduce mayor variabilidad en las respuestas, lo que promueve resultados más variados o creativos. Básicamente, al elevar la temperatura, se incrementa la posibilidad de elegir diferentes tokens como opciones. Desde un punto de vista práctico, en tareas como preguntas y respuestas basadas en información objetiva, se sugiere utilizar un valor de temperatura reducido para lograr respuestas más certeras y breves. Sin embargo, en la creación de poesía o en otras actividades creativas, puede resultar ventajoso aumentar el valor de la temperatura. El aumento en la temperatura puede generar alucinaciones.\n",
    "\n",
    "**Top P**: Esta es una técnica de muestreo en conjunto con la temperatura, conocida como muestreo de núcleo, que te permite controlar cuán determinista es el modelo. Si buscas respuestas exactas y basadas en hechos, es aconsejable mantener un valor bajo de Top P. Si, por el contrario, deseas respuestas más variadas, puedes aumentar este valor. Utilizar Top P significa que solo se consideran para las respuestas los tokens que constituyen la masa de probabilidad top_p, por lo que un valor bajo de top_p seleccionará las respuestas más seguras. Un valor alto de top_p, en cambio, permitirá que el modelo considere un rango más amplio de palabras posibles, incluyendo aquellas menos probables, lo que conduce a resultados más variados. La recomendación general es modificar la temperatura o el Top P, pero no ambos al mismo tiempo.\n",
    "\n",
    "El **Top K** es otro parámetro importante en la generación de texto por parte de los Modelos de Lenguaje de Gran Escala (LLM). Este parámetro se refiere a la selección de los K tokens más probables como candidatos para el siguiente token en la secuencia. Al ajustar el valor de K, los usuarios pueden controlar el grado de determinismo en las respuestas generadas por el modelo. Un valor de K más pequeño resultará en respuestas más deterministas y enfocadas, mientras que un valor de K más grande permitirá una mayor diversidad y creatividad en las respuestas al considerar un rango más amplio de tokens. \n",
    "\n",
    "La diferencia principal entre el Top P y el Top K radica en la forma en que ambos parámetros restringen el conjunto de tokens considerados para la generación del siguiente token en un Modelo de Lenguaje de Gran Escala (LLM).\n",
    "\n",
    "Top P se basa en la probabilidad acumulada de los tokens. En lugar de seleccionar un número fijo de tokens más probables (como hace el Top K), el Top P considera todos los tokens cuya probabilidad acumulada sea igual o inferior al valor de P establecido. Esto significa que el número de tokens considerados puede variar, dependiendo de las probabilidades asignadas a cada token por el modelo.\n",
    "\n",
    "Por otro lado, el Top K selecciona un número fijo de tokens más probables (K) como candidatos para el siguiente token. Al ajustar el valor de K, se limita el conjunto de tokens candidatos a los K tokens con mayor probabilidad.\n",
    "\n",
    "**Longitud Máxima(Max Length)**: Puedes gestionar el número de tokens que el modelo genera ajustando la longitud máxima. Especificar una longitud máxima te ayuda a evitar respuestas demasiado largas o irrelevantes y a controlar los costos asociados.\n",
    "\n",
    "**Secuencias de Parada(Stop Sequences)**: Una secuencia de parada es una cadena de texto que detiene la generación de tokens por parte del modelo. Especificar secuencias de parada es otra manera de controlar la longitud y estructura de las respuestas del modelo. Por ejemplo, puedes indicar al modelo que genere listas que no tengan más de 10 elementos añadiendo \"11\" como secuencia de parada.\n",
    "\n",
    "**Penalización por Frecuencia(Frequency Penalty)**: Esta penalización se aplica al siguiente token proporcionalmente a la cantidad de veces que ese token ya ha aparecido en la respuesta y en la indicación. Cuanto mayor sea la penalización por frecuencia, menos probable será que una palabra aparezca de nuevo. Esta configuración reduce la repetición de palabras en la respuesta del modelo al aplicar una penalización mayor a los tokens que aparecen con más frecuencia.\n",
    "\n",
    "**Penalización por Presencia(Presence Penalty)**: Al igual que la penalización por frecuencia, la penalización por presencia se aplica a los tokens repetidos, pero a diferencia de la primera, la penalización es la misma para todos los tokens repetidos, independientemente de la frecuencia de su aparición. Esto evita que el modelo repita frases con demasiada frecuencia en su respuesta. Si deseas que el modelo genere texto diverso o creativo, podrías optar por usar una penalización por presencia más alta. Por otro lado, si necesitas que el modelo mantenga el enfoque, podrías probar con una penalización por presencia más baja.\n",
    "\n",
    "Al igual que con la temperatura y el Top P, la recomendación general es modificar la penalización por frecuencia o por presencia, pero no ambas al mismo tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup del entorno y de la apikey de OpenAI y LLMCLoud"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:Teoría/01-Iniciacion.ipynb
   "execution_count": 10,
=======
   "execution_count": 11,
>>>>>>> origin/main:01-Iniciacion.ipynb
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "import sys\n",
    "sys.path.append('../utils/')\n",
    "from funciones import preguntar_llama, preguntar_gpt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante todo el curso vamos a utilizar tanto GPT-3.5-turbo de OpenAI (de pago), como LLAMA (gratuito). Para poder utilizarlos necesitamos una apikey que se obtiene a través de la web de [OpenAI](https://platform.openai.com/settings/profile?tab=api-keys) y la web [LLMCloud](https://www.llmcloud.app/keys) y que vamos a guardar en un archivo ```.env```. Para ello, vamos a instalar la librería python-dotenv que nos permitirá cargar las variables de entorno desde un archivo ```.env```. Dentro de ese archivo vamos a guardar tanto la apikey de OpenAI como la de LLMCloud de esta forma: ```OPENAI_API_KEY = tu_apikey```, ```AUTH_TOKEN = tu_apikey```.\n",
    "\n",
    "\n",
    "\n",
    "Vamos a crear también unas funciones que nos ayuden a la hora de usar los prompts para que sea más sencillo."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:Teoría/01-Iniciacion.ipynb
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intento 1 fallido. Error: 500 Server Error: Internal Server Error for url: https://api.awanllm.com/v1/chat/completions\n",
      "No se pudo completar la solicitud después de varios intentos.\n"
     ]
    },
    {
     "data": {
=======
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para llamar a la API de LLMCloud para preguntarle a LLAMA\n",
    "def preguntar_llama(sys_prompt, prompt):\n",
    "    payload = json.dumps({\n",
    "        \"model\": \"Meta-Llama-3-8B-Instruct\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": sys_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "      })\n",
    "    return requests.request(\"POST\", url, headers=headers, data=payload).json()['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "¡Hola! Soy un asistente de inteligencia artificial (IA) entrenado para responder a preguntas y proporcionar información sobre una variedad de temas. Mi función principal es procesar texto y generar respuestas basadas en los datos que he sido entrenado con.\n",
       "\n",
       "Puedo responder preguntas variadas, desde temas generales como historia, ciencias, tecnología y cultura hasta temas más específicos como definiciones, traducciones, conversaciones y más. Estoy diseñado para ser lo más preciso posible en mis respuestas, pero también estoy programado para aprender y mejorar con cada interacción que tengo con usuarios como tú.\n",
       "\n",
       "Además, puedo realizar tareas adicionales como generar textos, resumir artículos, ofrecer sugerencias de lectura o incluso crear historias cortas. ¡Si tienes alguna pregunta o tarea en mente, no dudes en preguntarme!"
      ],
>>>>>>> origin/main:01-Iniciacion.ipynb
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(preguntar_llama(sys_prompt='Eres un asistente de IA cuyo trabajo es responder a lo que te está pidiendo el usuario de la forma más fiel al prompt', prompt='¿Cuál es tu función?')))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:Teoría/01-Iniciacion.ipynb
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Respuesta:** ¡Hola! Estoy aquí para ayudarte en lo que necesites. Puedo responder a tus preguntas, ofrecerte información, brindarte sugerencias, entre otras cosas. ¿En qué puedo ayudarte hoy?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
=======
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'OpenAI'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Función para llamar a la GPT-3.5 de OpenAI\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpenAI\u001b[49m()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_completion\u001b[39m(prompt, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo-0125\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      5\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}]\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'openai' has no attribute 'OpenAI'"
     ]
>>>>>>> origin/main:01-Iniciacion.ipynb
    }
   ],
   "source": [
    "# Definimos el prompt\n",
    "prompt = \"Hola, qué puedes hacer por mí?\"\n",
    "\n",
    "# Obtenemos la respuesta\n",
    "response = preguntar_gpt(prompt=prompt, sys_prompt=\"Eres un asistente de IA cuyo trabajo es responder a lo que me estás pidiendo de la forma más fiel al prompt\")\n",
    "display(Markdown(f\"**Respuesta:** {response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
