{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iniciación al Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los Modelos de Lenguaje de Gran Escala (LLM) son sistemas de inteligencia artificial diseñados para generar texto humano coherente y contextualmente relevante. Para generar el siguiente token en una secuencia, los LLM utilizan una serie de parámetros que influyen en la selección del token adecuado. Conocer y ajustar estos parámetros es crucial para obtener resultados óptimos y adaptados a diferentes contextos y tareas.\n",
    "\n",
    "Algunos de los parámetros clave en la generación de tokens incluyen la temperatura, que determina el grado de aleatoriedad en la selección de tokens, y el Top P, que controla la probabilidad acumulada de los tokens considerados. Además, la longitud máxima y las secuencias de parada ayudan a controlar la extensión y estructura de las respuestas generadas. También existen penalizaciones por frecuencia y presencia para evitar la repetición innecesaria de palabras o frases.\n",
    "\n",
    "Entender y ajustar estos parámetros permite a los usuarios adaptar el comportamiento del LLM a diferentes aplicaciones, como responder preguntas basadas en hechos, generar contenido creativo o realizar traducciones automáticas, entre otras. Por lo tanto, es esencial familiarizarse con estos parámetros para aprovechar al máximo las capacidades de un LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El papel de los parámetros en los LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Temperatura (Temperature):** En esencia, a menor temperatura en un Modelo de Lenguaje de Gran Escala (LLM), los resultados tienden a ser más deterministas. Esto implica que el modelo optará con más frecuencia por el siguiente token de mayor probabilidad. Incrementar la temperatura introduce mayor variabilidad en las respuestas, lo que promueve resultados más variados o creativos. Básicamente, al elevar la temperatura, se incrementa la posibilidad de elegir diferentes tokens como opciones. Desde un punto de vista práctico, en tareas como preguntas y respuestas basadas en información objetiva, se sugiere utilizar un valor de temperatura reducido para lograr respuestas más certeras y breves. Sin embargo, en la creación de poesía o en otras actividades creativas, puede resultar ventajoso aumentar el valor de la temperatura. El aumento en la temperatura puede generar alucinaciones.\n",
    "\n",
    "**Top P**: Esta es una técnica de muestreo en conjunto con la temperatura, conocida como muestreo de núcleo, que te permite controlar cuán determinista es el modelo. Si buscas respuestas exactas y basadas en hechos, es aconsejable mantener un valor bajo de Top P. Si, por el contrario, deseas respuestas más variadas, puedes aumentar este valor. Utilizar Top P significa que solo se consideran para las respuestas los tokens que constituyen la masa de probabilidad top_p, por lo que un valor bajo de top_p seleccionará las respuestas más seguras. Un valor alto de top_p, en cambio, permitirá que el modelo considere un rango más amplio de palabras posibles, incluyendo aquellas menos probables, lo que conduce a resultados más variados. La recomendación general es modificar la temperatura o el Top P, pero no ambos al mismo tiempo.\n",
    "\n",
    "El **Top K** es otro parámetro importante en la generación de texto por parte de los Modelos de Lenguaje de Gran Escala (LLM). Este parámetro se refiere a la selección de los K tokens más probables como candidatos para el siguiente token en la secuencia. Al ajustar el valor de K, los usuarios pueden controlar el grado de determinismo en las respuestas generadas por el modelo. Un valor de K más pequeño resultará en respuestas más deterministas y enfocadas, mientras que un valor de K más grande permitirá una mayor diversidad y creatividad en las respuestas al considerar un rango más amplio de tokens. \n",
    "\n",
    "La diferencia principal entre el Top P y el Top K radica en la forma en que ambos parámetros restringen el conjunto de tokens considerados para la generación del siguiente token en un Modelo de Lenguaje de Gran Escala (LLM).\n",
    "\n",
    "Top P se basa en la probabilidad acumulada de los tokens. En lugar de seleccionar un número fijo de tokens más probables (como hace el Top K), el Top P considera todos los tokens cuya probabilidad acumulada sea igual o inferior al valor de P establecido. Esto significa que el número de tokens considerados puede variar, dependiendo de las probabilidades asignadas a cada token por el modelo.\n",
    "\n",
    "Por otro lado, el Top K selecciona un número fijo de tokens más probables (K) como candidatos para el siguiente token. Al ajustar el valor de K, se limita el conjunto de tokens candidatos a los K tokens con mayor probabilidad.\n",
    "\n",
    "**Longitud Máxima(Max Length)**: Puedes gestionar el número de tokens que el modelo genera ajustando la longitud máxima. Especificar una longitud máxima te ayuda a evitar respuestas demasiado largas o irrelevantes y a controlar los costos asociados.\n",
    "\n",
    "**Secuencias de Parada(Stop Sequences)**: Una secuencia de parada es una cadena de texto que detiene la generación de tokens por parte del modelo. Especificar secuencias de parada es otra manera de controlar la longitud y estructura de las respuestas del modelo. Por ejemplo, puedes indicar al modelo que genere listas que no tengan más de 10 elementos añadiendo \"11\" como secuencia de parada.\n",
    "\n",
    "**Penalización por Frecuencia(Frequency Penalty)**: Esta penalización se aplica al siguiente token proporcionalmente a la cantidad de veces que ese token ya ha aparecido en la respuesta y en la indicación. Cuanto mayor sea la penalización por frecuencia, menos probable será que una palabra aparezca de nuevo. Esta configuración reduce la repetición de palabras en la respuesta del modelo al aplicar una penalización mayor a los tokens que aparecen con más frecuencia.\n",
    "\n",
    "**Penalización por Presencia(Presence Penalty)**: Al igual que la penalización por frecuencia, la penalización por presencia se aplica a los tokens repetidos, pero a diferencia de la primera, la penalización es la misma para todos los tokens repetidos, independientemente de la frecuencia de su aparición. Esto evita que el modelo repita frases con demasiada frecuencia en su respuesta. Si deseas que el modelo genere texto diverso o creativo, podrías optar por usar una penalización por presencia más alta. Por otro lado, si necesitas que el modelo mantenga el enfoque, podrías probar con una penalización por presencia más baja.\n",
    "\n",
    "Al igual que con la temperatura y el Top P, la recomendación general es modificar la penalización por frecuencia o por presencia, pero no ambas al mismo tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup del entorno y de la apikey de OpenAI y LLMCLoud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import requests\n",
    "import os \n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante todo el curso vamos a utilizar tanto GPT-4o-mini de OpenAI (de pago), como LLAMA (gratuito). Para poder utilizarlos necesitamos una apikey que se obtiene a través de la web de [OpenAI](https://platform.openai.com/settings/profile?tab=api-keys) y la web [LLMCloud](https://www.llmcloud.app/keys) y que vamos a guardar en un archivo ```.env```. Para ello, vamos a instalar la librería python-dotenv que nos permitirá cargar las variables de entorno desde un archivo ```.env```. Dentro de ese archivo vamos a guardar tanto la apikey de OpenAI como la de LLMCloud de esta forma: \n",
    "```\n",
    "OPENAI_API_KEY = tu_apikey\n",
    "```\n",
    "```\n",
    "AUTH_TOKEN = tu_apikey\n",
    "```\n",
    "\n",
    "Vamos a crear también unas funciones que nos ayuden a la hora de usar los prompts para que sea más sencillo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función mejorada para consultar la API de LLMCloud utilizando el modelo \"Meta-Llama-3-8B-Instruct\".\n",
    "# Esta función envía una solicitud POST con un mensaje de configuración (sys_prompt) y un mensaje de usuario (prompt).\n",
    "# Se espera que la respuesta JSON contenga una lista en 'choices' y se obtiene el contenido del primer mensaje generado.\n",
    "\n",
    "# Se utiliza el token de autenticación obtenido de las variables de entorno 'AUTH_TOKEN'\n",
    "auth_token = os.getenv(\"AUTH_TOKEN\")\n",
    "\n",
    "# URL y cabeceras para la petición a la API\n",
    "url = \"https://api.awanllm.com/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {auth_token}\"\n",
    "}\n",
    "\n",
    "def preguntar_llama(sys_prompt, prompt):\n",
    "    \"\"\"\n",
    "    Función para llamar a la API de LLMCloud.\n",
    "\n",
    "    Parámetros:\n",
    "    sys_prompt (str): Mensaje del sistema que define el comportamiento del modelo.\n",
    "    prompt (str): Mensaje del usuario que se envía al modelo.\n",
    "\n",
    "    Retorna:\n",
    "    str: Respuesta generada por el modelo.\n",
    "    \"\"\"\n",
    "    # Configurar la carga útil (payload) para la solicitud\n",
    "    payload = {\n",
    "        \"model\": \"Meta-Llama-3-8B-Instruct\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": sys_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Convertir el payload a formato JSON\n",
    "    payload_json = json.dumps(payload)\n",
    "    \n",
    "    # Realizar la petición POST a la API\n",
    "    respuesta = requests.post(url, headers=headers, data=payload_json)\n",
    "    \n",
    "    # Recuperar y retornar el contenido de la respuesta.\n",
    "    return respuesta.json()['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Puedo realizar una amplia gama de tareas, como responder a preguntas, proporcionar información general en diversas categorías, generando texto, imagen y vídeo."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(preguntar_llama(sys_prompt=\"Eres un asistente de IA útil.\", prompt='¿Cuál es tu función?')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Respuesta:** ¡Hola! Puedo ayudarte con una variedad de tareas, como responder preguntas, proporcionar información sobre diferentes temas, ofrecer consejos, ayudarte a redactar textos o correos, y mucho más. ¿En qué específico necesitas ayuda?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# Inicializamos el cliente de OpenAI utilizando la API Key almacenada en la variable de entorno \"OPENAI_API_KEY\"\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Prompt del usuario: mensaje que se enviará a la API\n",
    "prompt = \"Hola, qué puedes hacer por mí?\"\n",
    "\n",
    "def preguntar_gpt(\n",
    "    prompt,\n",
    "    sys_prompt=\"Eres un asistente útil.\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    max_retries=3,\n",
    "    timeout=60,\n",
    "    rate_limit_delay=5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Envía una solicitud a la API de OpenAI usando el modelo y parámetros indicados.\n",
    "    \n",
    "    Parámetros:\n",
    "      prompt (str): Mensaje del usuario.\n",
    "      sys_prompt (str): Mensaje de sistema que define el comportamiento del modelo.\n",
    "      model (str): Modelo de OpenAI a utilizar (ej. \"gpt-4o-mini\").\n",
    "      temperature (float): Controla la aleatoriedad en la generación del texto.\n",
    "      max_retries (int): Número máximo de reintentos en caso de error.\n",
    "      timeout (int): Tiempo máximo en segundos para esperar la respuesta.\n",
    "      rate_limit_delay (int): Tiempo de espera en segundos al alcanzar el límite de solicitudes.\n",
    "    \n",
    "    Retorna:\n",
    "      str o None: Respuesta generada por la API o None si se agotan los intentos.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                timeout=timeout,\n",
    "            )\n",
    "            # Se retorna el contenido del primer mensaje de la respuesta.\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Intento {attempt + 1} fallido: {e}. Reintentando en {rate_limit_delay} segundos...\"\n",
    "            )\n",
    "            time.sleep(rate_limit_delay)\n",
    "\n",
    "    print(\"No se pudo obtener una respuesta tras varios intentos.\")\n",
    "    return None\n",
    "\n",
    "# Solicitamos la respuesta a la API con un sys_prompt más descriptivo\n",
    "response = preguntar_gpt(\n",
    "    prompt=prompt,\n",
    "    sys_prompt=(\"Eres un asistente de IA cuyo trabajo es responder a lo que se te está pidiendo de la forma más fiel al prompt\")\n",
    ")\n",
    "\n",
    "display(Markdown(f\"**Respuesta:** {response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez ya tenemos las funciones que nos permiten hacer peticiones contra LLAMA y GPT-4o-mini vamos a extraerlas a un archivo llamado ```utils/funciones.py``` para que sea más sencillo su uso en los siguientes notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
